# -*- coding: utf-8 -*-
"""topic_modeling_using_LSI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14v-uOVRAsxRCjviqq8ocFgtAT8jB5yRJ
"""

from gensim.models import LsiModel
import pandas as pd
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
from pprint import pprint
np.random.seed(2018)

data = pd.read_csv('https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv', error_bad_lines=False)
data_text = data[['headline_text']]
data_text['index'] = data_text.index
documents = data_text
documents

print(len(documents))
print(documents.head())
stemmer = PorterStemmer()

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

import nltk
nltk.download('wordnet')

# perform lemmatize and stem preprocessing steps on the data set.
doc_sample = documents[documents['index'] == 4310].values[0][0]
print('original document: ')
words = []
for word in doc_sample.split(' '):
    words.append(word)
print(words)
print('\n\n tokenized and lemmatized document: ')
print(preprocess(doc_sample))

# Preprocess the headline text, saving the results as ‘processed_docs’
processed_docs = documents['headline_text'].map(preprocess)
processed_docs.head(10)

# Bag of Words on the Data set
dictionary = gensim.corpora.Dictionary(processed_docs)
count = 0
for k, v in dictionary.iteritems():
    print(k, v)
    count += 1
    if count > 10:
        break

# Filter out tokens that appear in
dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)

# Gensim doc2bow
bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]

# Build the LSI Model
lsi_model = LsiModel(corpus=bow_corpus, id2word=dictionary, num_topics=7, decay=0.5)

# View Topics
pprint(lsi_model.print_topics(-1))

from gensim.models.word2vec import Word2Vec
from multiprocessing import cpu_count
import gensim.downloader as api

# Download dataset
dataset = api.load("text8")
data = [d for d in dataset]

# Split the data into 2 parts. Part 2 will be used later to update the model
data_part1 = data[:1000]
data_part2 = data[1000:]

# Train Word2Vec model. Defaults result vector size = 100
model = Word2Vec(data_part1, min_count = 0, workers=cpu_count())

# get the word vector for given word
model["topic"]

model.most_similar("topic")

# save model
model.save("/content/drive/MyDrive/LSIModel")
# load model
model = Word2Vec.load("/content/drive/MyDrive/LSIModel")

# update an existing Word2Vec model with new data
model.build_vocab(data_part2, update=True)
model.train(data_part2, total_examples=model.corpus_count, epochs=model.iter)
model['topic']

# extract word vectors using pre-trained Word2Vec and FastText models
import gensim.downloader as api

# Download the models
fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')
# word2vec_model300 = api.load('word2vec-google-news-300')
# glove_model300 = api.load('glove-wiki-gigaword-300')

# get word embeddings
word2vec_model300.most_similar('support')

# Word2Vec_accuracy
word2vec_model300.evaluate_word_analogies(analogies="/content/drive/MyDrive/questions-words.txt")[0]

# fasttext_accuracy
fasttext_model300.evaluate_word_analogies(analogies="/content/drive/MyDrive/questions-words.txt")[0]

# GloVe accuracy
glove_model300.evaluate_word_analogies(analogies="/content/drive/MyDrive/questions-words.txt")[0]

# create document vectors using Doc2Vec
import gensim
import gensim.downloader as api

# download dataset
dataset = api.load('text8')
data = [d for d in dataset]

# create the tagged document needed for Doc2Vec
def create_tagged_document(list_of_list_of_words):
  for i, list_of_words in enumerate(list_of_list_of_words):
    yield gensim.models.doc2vec.TaggedDocument(list_of_words,[i])

train_data = list(create_tagged_document(data))

print(train_data[:1])

# init the DOc2Vec model
model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)

# build the vocabulary
model.build_vocab(train_data)

# train the Doc2Vec model
model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)

print(model.infer_vector(['australian', 'captain', 'elected', 'to', 'bowl']))

# compute similarity metrics like cosine similarity and soft cosine similarity
from gensim.matutils import softcossim
from gensim import corpora

sent_1 = 'Sachin is a cricket player and a opening batsman'.split()
sent_2 = 'Dhoni is a cricket player too He is a batsman and keeper'.split()
sent_3 = 'Anand is a chess player'.split()

# prepare the similarity matrix
similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)

# pair a dictionary and a corpus
nents = [sent_1, sent_2, sent_3]
nary = corpora.Dictionary()

# Convert the sentences into bag-of-words vectors.
sent_1 = dictionary.doc2bow(sent_1)
sent_2 = dictionary.doc2bow(sent_2)
sent_3 = dictionary.doc2bow(sent_3)

# compute soft cosine similarity
print(softcossim(sent_1, sent_2, similarity_matrix))
print(softcossim(sent_1, sent_3, similarity_matrix))
print(softcossim(sent_2, sent_3, similarity_matrix))

# Which word from the given list doesn't go with the others?
print(fasttext_model300.doesnt_match(['india', 'australia', 'pakistan', 'china', 'beetroot']))  

# Compute cosine distance between two words.
print(fasttext_model300.distance('king', 'queen'))

# Compute cosine distances from given word or vector to all words in `other_words`.
print(fasttext_model300.distances('king', ['queen', 'man', 'woman']))

# Compute cosine similarities
print(fasttext_model300.cosine_similarities(fasttext_model300['king'], 
                                            vectors_all=(fasttext_model300['queen'], 
                                                        fasttext_model300['man'], 
                                                        fasttext_model300['woman'],
                                                        fasttext_model300['queen'] + fasttext_model300['man'])))  

# Get the words closer to w1 than w2
print(glove_model300.words_closer_than(w1='king', w2='kingdom'))

# Find the top-N most similar words.
print(fasttext_model300.most_similar(positive='king', negative=None, topn=5, restrict_vocab=None, indexer=None))

# Find the top-N most similar words, using the multiplicative combination objective,
print(glove_model300.most_similar_cosmul(positive='king', negative=None, topn=5))

# summarize text documents

from gensim.summarization import summarize, keywords
from pprint import pprint
from smart_open import smart_open

text = " ".join((line for line in smart_open('/content/drive/MyDrive/questions-words.txt', encoding='utf-8')))

# Summarize the paragraph
pprint(summarize(text, word_count=200))

# Important keywords from the paragraph
print(keywords(text))

